{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a0fa14",
   "metadata": {},
   "source": [
    "# Training and application of the Deconfounding DualADAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c21752",
   "metadata": {},
   "source": [
    "The following notebook provides the instructions to create and train from scratch a DualADAE model on multi-center and multi-scanner radiomics data.\n",
    "It requires the DualADAE repository installed in the system that is running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52ed4c",
   "metadata": {},
   "source": [
    "First we will import the radiomics dataset, that has to be stored in the `DATA/` folder in csv format.\n",
    "The `csv_path` variable is loaded automatically from `utils.py` script. If you want to set a different directory for your data, change the definition of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72725d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data to process\n",
    "filepath = csv_path + 'data.csv' # data.csv contains your radiomics matrix\n",
    "_data_df = pd.read_csv(filepath)\n",
    "\n",
    "# Scale your data\n",
    "data_df_std = StandardScaler().fit_transform(_data_df)\n",
    "data_df = pd.DataFrame(data_df_tmp, columns=_data_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d320c9",
   "metadata": {},
   "source": [
    "## Model Architecture and Hyperparameters Optimization\n",
    "\n",
    "The DualADAE library allows the user to define a set of hyperparameters (number of nodes per hidden layer and $p$ value for the drop-out) and performs grid search of the best Autoencoder model configuration. The best model is identified minimizing the AE reconstruction error.\n",
    "\n",
    "By default, the library tests three versions of the AE model:\n",
    "- Three layers AE\n",
    "- Two layers AE\n",
    "- One layer AE\n",
    "\n",
    "\n",
    "The function `make_AE_optimization()` will take as input the radiomics dataset and 4 lists of hyperparameters: \n",
    "- dimensions of the first hidden layer (`dim1`)\n",
    "- dimensions of the second hidden layer (`dim2`)\n",
    "- dimensions of the third hidden layer (`dim3`)\n",
    "- set of drop-out values (`dropouts`)\n",
    "\n",
    "Out of this four lists, it will generate **three `.json` files**, that will be stored in the `PARAMS/` folder: each file contains a library of the best configuration for each model version. \n",
    "\n",
    "\n",
    "\n",
    "**Note** Model optimization can be skipped by moving directly to the next section. The model in this case will be defined on the basis of a default set of hyperparameters, saved as `default_model.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to evaluate in grid search of the best model configuration\n",
    "dims1 = [32, 16, 8]\n",
    "dims2 = [16, 8, 4]\n",
    "dims3 = [8, 4, 2]\n",
    "dropouts = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Perform model optimization\n",
    "make_AE_optimization(data_df, dims1, dims2, dims3, dropouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71e5a6",
   "metadata": {},
   "source": [
    "## DualADAE training and deconfusion of radiomics data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d6f5b",
   "metadata": {},
   "source": [
    "First we have to import the confounding factors data, and prepare the training and test sets to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecb3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data to process\n",
    "filename_center = csv_path + 'center.csv' # center.csv contains your center labels\n",
    "center_df = pd.read_csv(filename_center)\n",
    "filename_scanner = csv_path + 'scanner.csv' # center.csv contains your center labels\n",
    "scanner_df = pd.read_csv(filename_scanner)\n",
    "\n",
    "labels_df = pd.concat([center_df, scanner_df], axis=1)\n",
    "\n",
    "# Detect common samples\n",
    "common_samples = np.intersect1d(data_df.index.values, labels_df.index.values)\n",
    "labels_df = labels_df.loc[common_samples]\n",
    "input_df = data_df.loc[common_samples]\n",
    "\n",
    "# prepare your final training data\n",
    "X = input_df\n",
    "Y = labels_df\n",
    "n_centers = len(np.unique(center_df))\n",
    "n_scanners =  len(np.unique(scanner_df))\n",
    "\n",
    "# Train and test splits\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2e8ae",
   "metadata": {},
   "source": [
    "The following is the actual training of the DualADAE model, calling the function `make_DualADAE()`.\n",
    "\n",
    "This function takes several inputs:\n",
    "- `X_train`, `Y_train`, `X_test`, `X_test` are the input datasets prepared above\n",
    "- `name_dict_param` is a string that contains the name of the chosen model version: users can choose between one-layer, two-layer or three-layer best configuration (in case they run the optimization step), or the default model.\n",
    "- `iterations`: number of training iterations\n",
    "- `n_centers`: the number of Centers that collected the data\n",
    "- `n_scanners`: the total number of scanners used among all centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07a1df",
   "metadata": {},
   "source": [
    "The function returns a dataframe containing the deconfounded embeddings. The same dataframe is by default stored in the `ADV_FILES_DUAL/` folder, that will be automatically created in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "lambda_val = 0.5\n",
    "name_dict_param = 'best_AE_param_two_layer.json' #change with 'default_model.json' in case no optimization was performed\n",
    "iternations = 6000\n",
    "\n",
    "embedding_df = make_DualADAE(X_train, \n",
    "                             Y_train, \n",
    "                             X_test, \n",
    "                             Y_test, \n",
    "                             name_dict_param, \n",
    "                             lambda_val, \n",
    "                             iternations, \n",
    "                             n_centers, \n",
    "                             n_scanners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fea69",
   "metadata": {},
   "source": [
    "The code above train the AD-AE model in a single-split validation model. If we want to train the network in cross-validation mode, we follow the code below, calling `make_DualADAE_crossval()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8effe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "lambda_val = 0.5\n",
    "name_dict_param = 'best_AE_param_two_layer.json' #change with 'default_model.json' in case no optimization was performed\n",
    "iternations = 100\n",
    "n_split = 50\n",
    "\n",
    "embedding_df = make_DualADAE_crossval(X, Y, \n",
    "                                      name_dict_param,\n",
    "                                      run,\n",
    "                                      lambda_val,\n",
    "                                      n_split,\n",
    "                                      iternations,\n",
    "                                      n_centers, \n",
    "                                      n_scanners)\n",
    "\n",
    "# Have fun with your embeddings!! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88817b8",
   "metadata": {},
   "source": [
    "where:\n",
    "- `X`, `Y`, `X_test`, `X_test` are the non-split input datasets\n",
    "- `iterations`: number of training iterations per each split\n",
    "- `n_split`: number of cross-validation split\n",
    "- and the rest of parameters as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5376d56",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
